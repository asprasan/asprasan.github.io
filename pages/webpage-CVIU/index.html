<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>High Frame Rate Optical Flow Estimation from Event Sensors via Intensity Estimation</title>
	<meta property="og:image" content="./resources/teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="High Frame Rate Optical Flow Estimation from Event Sensors via Intensity Estimation" />
	<meta property="og:description" content="A learning based framework for joint reconstruction of intensity image and optical flow from event sensors." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">High Frame Rate Optical Flow Estimation from Event Sensors via Intensity Estimation</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://asprasan.github.io">Prasan Shedligeri</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="http://www.ee.iitm.ac.in/kmitra/">Kaushik Mitra</a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='./resources/paper.pdf'>[Paper]</a></span>
						</center>
					</td>
					<!-- <td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/asprasan/codedblurred'>[GitHub]</a></span><br>
						</center>
					</td> -->
				</tr>
			</table>
		</table>
	</center>

	<center>
		<!-- <table align=center width=850px> -->
			<style type="text/css">
.tg  {border:none;border-collapse:collapse;border-spacing:0;}
.tg td{border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;
  padding:5px 5px;word-break:normal;}
.tg th{border-style:solid;border-width:0px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-8jgo{border-color:#ffffff;text-align:center;vertical-align:center}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:center}
</style>
<table class="tg">
<!-- <thead>
  <tr>
    <th class="tg-8jgo"></th>
    <th class="tg-8jgo"></th>
    <th class="tg-8jgo">Coded</th>
    <th class="tg-c3ow" colspan="2">Fully exposed - coded (Ours)</th>
  </tr>
</thead> -->
<tbody>
  <tr>
    <td class="tg-8jgo"><SPAN STYLE="writing-mode: vertical-lr;
                     -ms-writing-mode: tb-rl;
                     transform: rotate(180deg);">Raw frame</SPAN></td>
    <td class="tg-8jgo"><img src="./resources/optical_flow/s0/frame.png" width="240" height="180"></td>
    <td class="tg-8jgo"><img src="./resources/optical_flow/s1/frame.png" width="240" height="180"></td>
    <td class="tg-c3ow"><img src="./resources/optical_flow/s2/frame.png" width="240" height="180"></td>
  </tr>
  <tr>
    <td class="tg-8jgo"><SPAN STYLE="writing-mode: vertical-lr;
                     -ms-writing-mode: tb-rl;
                     transform: rotate(180deg);">Event frame</SPAN></td>
    <td class="tg-8jgo"><img src="./resources/optical_flow/s0/events.png" width="240" height="180"></td>
    <td class="tg-8jgo"><img src="./resources/optical_flow/s1/events.jpg" width="240" height="180"></td>
    <td class="tg-8jgo"><img src="./resources/optical_flow/s2/events.jpg" width="240" height="180"></td>
  </tr>
  <tr>
    <td class="tg-8jgo"><SPAN STYLE="writing-mode: vertical-lr;
                     -ms-writing-mode: tb-rl;
                     transform: rotate(180deg);">Predicted flow</SPAN></td>
    <td class="tg-8jgo"><img src="./resources/optical_flow/s0/flow.gif" width="240" height="180"></td>
    <td class="tg-8jgo"><img src="./resources/optical_flow/s1/flow.gif" width="240" height="180"></td>
    <td class="tg-8jgo"><img src="./resources/optical_flow/s2/flow.gif" width="240" height="180"></td>
  </tr>
  <tr>
    <td class="tg-8jgo"><SPAN STYLE="writing-mode: vertical-lr;
                     -ms-writing-mode: tb-rl;
                     transform: rotate(180deg);">Predicted video</SPAN></td>
    <td class="tg-8jgo"><img src="./resources/optical_flow/s0/frame.gif" width="240" height="180"></td>
    <td class="tg-8jgo"><img src="./resources/optical_flow/s1/frame.gif" width="240" height="180"></td>
    <td class="tg-8jgo"><img src="./resources/optical_flow/s2/frame.gif" width="240" height="180"></td>
  </tr>
</tbody>
</table>
			<!-- <tr>
				<td width=260px>
					<center>
						<img class="round" style="width:500px" src="./resources/fusion_teaser.png"/>
					</center>
				</td>
			</tr> -->
		<!-- </table> -->
		<!-- <table align=center width=850px>
			<tr>
				<td>
					This was a template originally made for <a href="http://richzhang.github.io/colorization/">Colorful Image Colorization</a>. The code can be found in this <a href="https://github.com/richzhang/webpage-template">repository</a>.
				</td>
			</tr>
		</table> -->
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td align=justify>
				Optical flow estimation forms the core of several computer vision tasks and its estimation requires accurate spatial and temporal gradient information.
				However, if there are fast-moving objects in the scene or if the camera moves rapidly, then the acquired images will suffer from motion blur, which will lead to poor optical flow estimation.
				Such challenging cases can be handled by event sensors which are a novel generation of sensors that acquire pixel-level brightness changes as binary events at a very high temporal resolution.
				Brightness constancy constraint, which is the basis of several optical flow algorithms cannot be directly used on event sensors making it challenging to estimate optical flow.
				We overcome this challenge by imposing brightness constancy constraint on intensity images predicted from event sensor data.
				For this task, we design a recurrent neural network that jointly predicts a sparse optical flow and intensity images from the event data.
				While intensity estimation is supervised using ground truth frames, optical flow estimation is self-supervised using the predicted intensity frames.
				However, in our case the temporal resolution of the ground truth intensity frames is far lower than the temporal resolution of the predicted intensity frames, making it challenging to supervise.
				As we use recurrent neural network, such a challenge can be overcome by sharing the weights for each of the predicted intensity frames.
				Quantitatively our predicted optical flow is better than previously proposed algorithms for optical flow estimation from event sensors.
				We also show our algorithm’s robustness against challenging cases of fast motion and high dynamic range scenes.
			</td>
		</tr>
	</table>
	<br>

	<hr>
	<center><h1>Talk</h1></center>
	<p align="center">
		<iframe src="https://drive.google.com/file/d/1wdkF1QapN1bSohgjKV9MqhBl6qrbNJMT/preview" width="660" height="395" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
	</p>

	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:28px"><a href='https://docs.google.com/presentation/d/16XcZHiEbf3CfBPd9aU2YyIW-hXrq8Nelwsfb4SY4_Fw/edit?usp=sharing'>[Slides]</a>
				</span>
			</center>
		</tr>
	</table>
	<hr>

	<center><h1>Key takeaways</h1></center>

	<table align=center width=800px>
		<center>
			<tr>
				<td>
					<li>
						We propose a semi-supervised learning algorithm to predict high frame rate optical flow for high dynamic range scenes.
					</li>
					<li>
						Optical flow prediction is self-supervised using the high frame rate and high dynamic range intensity frames predicted directly from the event sensor data. Thus, ground truth optical flow is not necessary for training our proposed algorithm.
					</li>
					<li>
						We also demonstrate the generalizability of our proposed algorithm on a wide variety of open source event datasets captured with different sensors and in different environments.
					</li>
				</td>
			</tr>
		</center>
	</table>

	<hr>

	<center><h1>Algorithm</h1></center>

	<!-- <table align=center width=640px>
		<center>
			<tr>
				<td>

				</td>
			</tr>
		</center>
	</table> -->
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:600px" src="./resources/method.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td align=justify>
					Overall flow of our proposed method: Our proposed methods takes in a single event frame at each time-step, which is then input to a ConvLSTM (Convolutional Long-Short Term Memory) network.
   					The updated hidden state from the convLSTM network is input to an encoder network consisting of four strided convolutional layers followed by a ResNet block.
   					The hidden representation from the encoder network is then fed as input to two decoder networks, decoderImg and decoderFlow, which predict the intensity image and the optical flow, respectively.
				</td>
			</tr>
		</center>
	</table>
	<!-- <table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/asprasan/codedblurred'>[GitHub]</a>
			</center>
		</span>
	</table> -->
	<br>
	<hr>
	<table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt"> P. Shedligeri, Kaushik Mitra<br>
				<b> High Frame Rate Optical Flow Estimation from Event Sensors via Intensity Estimation</b><br>
				Under review<br>
				(Available as <a href="./resources/paper.pdf">Pre-print</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=300px>
		<tr>
			<td align=center><span style="font-size:14pt">
				<a href="https://docs.google.com/presentation/d/16XcZHiEbf3CfBPd9aU2YyIW-hXrq8Nelwsfb4SY4_Fw/edit?usp=sharing">[Supplementary Material]</a></td>
			<!-- <td align=center ><span style="font-size:14pt">
				<a href="./resources/bibtex.txt">[Bibtex]</a></td> -->
		</tr>
	</table>

	<hr>
	<br>

	<center><h1>Related Publications</h1></center>

	<table align=center width=900px>
		<center>
			<tr>
				<td>
					<li><b>Prasan Shedligeri</b>, Anupama S & Kaushik Mitra. (2021) A Unified Framework for Compressive Video Recovery from Coded ExposureTechniques. Accepted at <em>IEEE/CVF Winter Conference on Applications of Computer Vision,</em> doi to be assigned 
            	<a href="https://arxiv.org/abs/2011.05532">[Preprint]</a>
            	<a href="https://docs.google.com/presentation/d/1TqyRWTtNqIssMJ_nnHTTNLNbdP_oMqGtBlgvnaNTcYI/edit?usp=sharing">[Slides]</a>
            	<a href="https://drive.google.com/file/d/1GGpgzTGXnE1XzONJOv81iK23TVIfkAvB/view?usp=sharing">[Supplementary]</a>
            	<a href="https://github.com/asprasan/unified_framework">[Code]</a>
            <a href="https://asprasan.github.io/unified_framework/">[Webpage]</a></li>

	           		<li><b>Prasan Shedligeri</b>, Anupama S & Kaushik Mitra. (2021) CodedRecon: Video reconstruction for coded exposure imaging techniques. Accepted at <em>Elsevier Journal of Software Impacts,</em> https://doi.org/10.1016/j.simpa.2021.100064
	            	<a href="https://www.sciencedirect.com/science/article/pii/S2665963821000129">[Paper]</a>
	            	<a href="https://github.com/asprasan/unified_framework">[Code]</a></li>
				</td>
			</tr>
		</center>
	</table>

	<hr>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					The authors would like to thank Matta Gopi Raju for collecting some of the data used in this and related publications.
					<br>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

